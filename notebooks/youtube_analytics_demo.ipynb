{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb9caea",
   "metadata": {},
   "source": [
    "Import libraries & start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session (Databricks does this automatically, but keeping for local runs)\n",
    "spark = SparkSession.builder.appName(\"YouTubeAnalytics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333163c",
   "metadata": {},
   "source": [
    "Load curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc57e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load transformed parquet data from S3\n",
    "df = spark.read.parquet(\"s3a://my-youtube-bucket/clean/youtube_videos\")\n",
    "\n",
    "# Display schema\n",
    "df.printSchema()\n",
    "\n",
    "# Quick sample\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676575be",
   "metadata": {},
   "source": [
    "Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of videos per channel\n",
    "df.groupBy(\"channel_title\") \\\n",
    "  .agg(F.count(\"*\").alias(\"video_count\")) \\\n",
    "  .orderBy(F.desc(\"video_count\")) \\\n",
    "  .show()\n",
    "\n",
    "# Find the most recent videos\n",
    "df.orderBy(F.col(\"published_at\").desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54e9e0",
   "metadata": {},
   "source": [
    "Add partitioning column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from published date for partitioning\n",
    "df_with_date = df.withColumn(\"year\", F.year(\"published_at\"))\n",
    "\n",
    "# Show sample\n",
    "df_with_date.select(\"video_id\", \"title\", \"year\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc8f88",
   "metadata": {},
   "source": [
    "Save partitioned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd452876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save partitioned by year for faster queries\n",
    "df_with_date.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(\"s3a://my-youtube-bucket/partitioned/youtube_videos\")\n",
    "\n",
    "print(\" Data saved partitioned by year\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d5380",
   "metadata": {},
   "source": [
    "Caching for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache DataFrame in memory for faster repeated queries\n",
    "df.cache()\n",
    "\n",
    "# First action triggers cache\n",
    "print(\"Cached count:\", df.count())\n",
    "\n",
    "# Re-run query (faster because data is cached)\n",
    "df.groupBy(\"channel_title\").agg(F.count(\"*\").alias(\"video_count\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8533e",
   "metadata": {},
   "source": [
    "Bucketing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d46cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as bucketed table (good for joins on channel_title)\n",
    "df.write \\\n",
    "    .bucketBy(4, \"channel_title\") \\\n",
    "    .sortBy(\"published_at\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"youtube_bucketed\")\n",
    "\n",
    "print(\"✅ Saved as bucketed table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c6f8c",
   "metadata": {},
   "source": [
    "Delta Lake storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Delta table (transactional, supports time travel)\n",
    "df_with_date.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .save(\"s3a://my-youtube-bucket/delta/youtube_videos\")\n",
    "\n",
    "print(\"✅ Delta table created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed2091",
   "metadata": {},
   "source": [
    "Delta Lake optimization (Z-Order) (If required to opmimize)\n",
    "Note-Optimize is more costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact small files into fewer large files and Z-Order by channel_title\n",
    "spark.sql(\"\"\"\n",
    "OPTIMIZE delta.`s3a://my-youtube-bucket/delta/youtube_videos`\n",
    "ZORDER BY (channel_title)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Delta table optimized with Z-Ordering\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9704f8f",
   "metadata": {},
   "source": [
    "Analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window functions (top N videos per channel)\n",
    "# Define window by channel and order by published date\n",
    "w = Window.partitionBy(\"channel_title\").orderBy(F.col(\"published_at\").desc())\n",
    "\n",
    "# Rank videos per channel\n",
    "df_ranked = df.withColumn(\"row_num\", F.row_number().over(w))\n",
    "\n",
    "# Get top 3 videos per channel\n",
    "df_ranked.filter(F.col(\"row_num\") <= 3).show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
